{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Neural Network using Keras and Sckit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.iloc[:,1:].values\n",
    "y = df_train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some random data\n",
    "idx = np.random.randint(low=0, high =X.shape[0], size=16)\n",
    "sel = y[idx]\n",
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = X[idx, :]\n",
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayData(X):\n",
    "    width=int(np.round(np.sqrt(X.shape[1])))\n",
    "    (m,n) = X.shape\n",
    "    height = int(n/width)\n",
    "    \n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m/display_rows))\n",
    "    \n",
    "    fig, axes = pp.subplots( nrows=display_rows, ncols=display_cols, figsize=(20,10))\n",
    "    pp.subplots_adjust(hspace = 0.01, wspace=0.01)\n",
    "    k = 0\n",
    "    for i in range(display_rows):\n",
    "        for j in range(display_cols):\n",
    "            axes[i,j].imshow(X[k].reshape(height, width), cmap='gray')\n",
    "            axes[i,j].set_xticks([])\n",
    "            axes[i,j].set_yticks([])\n",
    "            axes[i,j].set_xticklabels([])\n",
    "            axes[i,j].set_yticklabels([])\n",
    "            k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayData(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try with a normal ANN (without data augumentation and convolution or regularization)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_cv = scaler.transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# For Multi class classification we need to one hot encode the labels (outputs)\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_cv = np_utils.to_categorical(y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target number of classes\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required keras classes for our keras model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = X.shape[1]\n",
    "def createANNModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createANNModel()\n",
    "model.fit(X_train, y_train, validation_data = (X_cv, y_cv), epochs=10, batch_size=100, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_cv, y_cv,verbose=0)\n",
    "print('Error :{0:0.2f} %'.format(100 - (scores[1]* 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Convolution Neural Network\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to rehsape our data in the following format\n",
    "width = int(np.round(np.sqrt(num_pixels)))\n",
    "height = int(np.round(num_pixels/width))\n",
    "\n",
    "# We already have scaled our data above\n",
    "# (samples, channels, width, height)\n",
    "# channel = 1 (grayscale), 3 (RGB), 4 (RGBA)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, width,height ))\n",
    "X_cv = X_cv.reshape((X_cv.shape[0], 1, width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5×5 and a rectifier activation function. This is the input layer, expecting images with the structure (channels, width, height)\n",
    "2. Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2×2.\n",
    "3. The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    "4. Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n",
    "5. Next a fully connected layer with 128 neurons and rectifier activation function.\n",
    "6. Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCNNModel():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5,5), strides=(1,1), data_format='channels_first', activation='relu', input_shape=(1, width, height)))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29400 samples, validate on 12600 samples\n",
      "Epoch 1/20\n",
      " - 54s - loss: 0.8068 - acc: 0.7752 - val_loss: 0.3354 - val_acc: 0.8995\n",
      "Epoch 2/20\n",
      " - 53s - loss: 0.2896 - acc: 0.9160 - val_loss: 0.2406 - val_acc: 0.9298\n",
      "Epoch 3/20\n",
      " - 54s - loss: 0.2078 - acc: 0.9412 - val_loss: 0.1771 - val_acc: 0.9483\n",
      "Epoch 4/20\n",
      " - 54s - loss: 0.1526 - acc: 0.9558 - val_loss: 0.1366 - val_acc: 0.9613\n",
      "Epoch 5/20\n",
      " - 53s - loss: 0.1170 - acc: 0.9676 - val_loss: 0.1191 - val_acc: 0.9656\n",
      "Epoch 6/20\n",
      " - 54s - loss: 0.0967 - acc: 0.9716 - val_loss: 0.0935 - val_acc: 0.9744\n",
      "Epoch 7/20\n",
      " - 53s - loss: 0.0811 - acc: 0.9763 - val_loss: 0.0832 - val_acc: 0.9765\n",
      "Epoch 8/20\n",
      " - 53s - loss: 0.0681 - acc: 0.9809 - val_loss: 0.0776 - val_acc: 0.9788\n",
      "Epoch 9/20\n",
      " - 53s - loss: 0.0614 - acc: 0.9826 - val_loss: 0.0712 - val_acc: 0.9800\n",
      "Epoch 10/20\n",
      " - 53s - loss: 0.0556 - acc: 0.9844 - val_loss: 0.0670 - val_acc: 0.9817\n",
      "Epoch 11/20\n",
      " - 53s - loss: 0.0491 - acc: 0.9854 - val_loss: 0.0638 - val_acc: 0.9826\n",
      "Epoch 12/20\n",
      " - 55s - loss: 0.0445 - acc: 0.9881 - val_loss: 0.0629 - val_acc: 0.9826\n",
      "Epoch 13/20\n",
      " - 56s - loss: 0.0412 - acc: 0.9883 - val_loss: 0.0642 - val_acc: 0.9831\n",
      "Epoch 14/20\n",
      " - 66s - loss: 0.0381 - acc: 0.9892 - val_loss: 0.0602 - val_acc: 0.9840\n",
      "Epoch 15/20\n",
      " - 98s - loss: 0.0337 - acc: 0.9907 - val_loss: 0.0588 - val_acc: 0.9842\n",
      "Epoch 16/20\n",
      " - 98s - loss: 0.0328 - acc: 0.9906 - val_loss: 0.0593 - val_acc: 0.9842\n",
      "Epoch 17/20\n",
      " - 98s - loss: 0.0290 - acc: 0.9920 - val_loss: 0.0574 - val_acc: 0.9844\n",
      "Epoch 18/20\n",
      " - 99s - loss: 0.0280 - acc: 0.9922 - val_loss: 0.0563 - val_acc: 0.9848\n",
      "Epoch 19/20\n",
      " - 98s - loss: 0.0261 - acc: 0.9926 - val_loss: 0.0577 - val_acc: 0.9843\n",
      "Epoch 20/20\n",
      " - 98s - loss: 0.0244 - acc: 0.9933 - val_loss: 0.0560 - val_acc: 0.9853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17a94001470>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = createCNNModel()\n",
    "model.fit(X_train, y_train, validation_data=(X_cv, y_cv), epochs=20, batch_size=1000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_cv, y_cv,verbose=0)\n",
    "print('Error :{0:0.2f} %'.format(100 - (scores[1]* 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Mnist_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our previously saved model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('Mnist_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv', dtype=np.int32)\n",
    "X_test = df_test.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= X_test.reshape((X_test.shape[0], 1, width, height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress scientific notations while priting\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "print(y_pred[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax( y_pred , axis =1)\n",
    "predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "submission = np.column_stack((np.arange(1, df_test.shape[0] + 1), predictions))\n",
    "submission = pd.DataFrame(data=submission, columns=['ImageId', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
